{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "437e04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import jit, grad, vmap\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322949cc",
   "metadata": {},
   "source": [
    "## jax/numpy difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24800cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Array: [0 1 2 3 4]\n",
      "Original JAX Array: [0 1 2 3 4]\n",
      "Updated JAX Array: [10  1  2  3  4]\n"
     ]
    }
   ],
   "source": [
    "def jax_basics():\n",
    "    x_np = np.arange(5)\n",
    "    x_jax = jnp.arange(5)\n",
    "\n",
    "    print(\"JAX Array:\", x_jax)\n",
    "    #x_jax[0] = 10 # illegal operation\n",
    "    y_jax = x_jax.at[0].set(10)\n",
    "\n",
    "    print(\"Original JAX Array:\", x_jax)\n",
    "    print(\"Updated JAX Array:\", y_jax)\n",
    "\n",
    "jax_basics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e7b98",
   "metadata": {},
   "source": [
    "## grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d44d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 1/(x^2), x = 2.0\n",
      "Gradient (-2/(x^3)) result: -0.25\n"
     ]
    }
   ],
   "source": [
    "def derivative_example(input_val):\n",
    "    def f(x):\n",
    "        return 1/(x**2)\n",
    "    \n",
    "    df_dx = grad(f)\n",
    "\n",
    "    result = df_dx(input_val)\n",
    "\n",
    "    print(f\"f(x) = 1/(x^2), x = {input_val}\")\n",
    "    print(f\"Gradient (-2/(x^3)) result: {result}\")\n",
    "\n",
    "derivative_example(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13269753",
   "metadata": {},
   "source": [
    "# jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7df43c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without JIT: 0.0144 seconds\n",
      "With JIT:    0.0115 seconds\n"
     ]
    }
   ],
   "source": [
    "def heavy_computation(x):\n",
    "    return jnp.dot(x, x.T)\n",
    "\n",
    "def jit_example():\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    x = jax.random.normal(key, (5000,5000))\n",
    "\n",
    "    start = time.time()\n",
    "    heavy_computation(x).block_until_ready() ## make sure \n",
    "    print(f\"Without JIT: {time.time() - start:.4f} seconds\")\n",
    "\n",
    "    fast_computation = jit(heavy_computation)\n",
    "    fast_computation(x).block_until_ready()\n",
    "\n",
    "    start = time.time()\n",
    "    fast_computation(x).block_until_ready()\n",
    "    print(f\"With JIT:    {time.time() - start:.4f} seconds\")\n",
    "\n",
    "jit_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b74b3bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "   JAX BENCHMARK: No-JIT vs. JIT\n",
      "==========================================\n",
      "\n",
      "--- 1. Baseline (No JIT / Eager) ---\n",
      "[No JIT - Data A] Time: 0.10474 s\n",
      "[No JIT - Data B] Time: 0.10433 s\n",
      "\n",
      "--- 2. JIT Compilation Phase ---\n",
      "[Compilation (Data A)] Time: 0.25615 s\n",
      "\n",
      "--- 3. Optimized Execution (JIT) ---\n",
      "[JIT - Data A] Time: 0.00111 s\n",
      "[JIT - Data B] Time: 0.00114 s\n",
      "\n",
      "==========================================\n",
      "             FINAL RESULTS                \n",
      "==========================================\n",
      "Data A Speedup: 94.53x  (Raw: 0.1047s -> JIT: 0.0011s)\n",
      "Data B Speedup: 91.89x  (Raw: 0.1043s -> JIT: 0.0011s)\n",
      "\n",
      "âœ… Conclusion: MASSIVE success. The loop fusion worked perfectly.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# --- 1. Setup Data ---\n",
    "# We use a reasonably sized array\n",
    "N = 2000\n",
    "data_A = jax.random.normal(jax.random.PRNGKey(0), (N, N))\n",
    "data_B = jax.random.normal(jax.random.PRNGKey(1), (N, N))\n",
    "\n",
    "# --- 2. The Logic: \"Loop Hell\" ---\n",
    "# Why this?\n",
    "# Without JIT: Python dispatches op-by-op. 100 loops * 3 ops = 300 overheads.\n",
    "# With JIT: XLA fuses all 300 ops into ONE single GPU/CPU kernel.\n",
    "def massive_loop_operation(x):\n",
    "    res = x\n",
    "    # A loop that is painful for Python interpreter but easy for XLA\n",
    "    for _ in range(100): \n",
    "        res = res - 0.001 * jnp.sin(res) # Element-wise math\n",
    "        res = jnp.where(res > 1.0, 1.0, res) # Conditional logic\n",
    "    return jnp.sum(res)\n",
    "\n",
    "# --- 3. Benchmark Helper (With Explicit Warmup) ---\n",
    "def measure_time(func, data, name):\n",
    "    # 1. Warmup / Dry Run (User's logic: \"Run once, disregard result\")\n",
    "    # This initializes any lazy allocations or caches\n",
    "    _ = func(data).block_until_ready()\n",
    "    \n",
    "    # 2. Actual Measurement\n",
    "    # Now that the system is \"warm\", we measure the second run\n",
    "    start = time.time()\n",
    "    res = func(data)\n",
    "    res.block_until_ready() # CRITICAL: Wait for async computation\n",
    "    end = time.time()\n",
    "    \n",
    "    duration = end - start\n",
    "    print(f\"[{name}] Time: {duration:.5f} s\")\n",
    "    return duration\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"   JAX BENCHMARK: No-JIT vs. JIT\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# --- Scenario 1: No JIT (Baseline) ---\n",
    "# We explicitly check BOTH Data A and Data B without JIT\n",
    "with jax.disable_jit():\n",
    "    print(\"--- 1. Baseline (No JIT / Eager) ---\")\n",
    "    t_raw_A = measure_time(massive_loop_operation, data_A, \"No JIT - Data A\")\n",
    "    t_raw_B = measure_time(massive_loop_operation, data_B, \"No JIT - Data B\")\n",
    "\n",
    "# --- Scenario 2: JIT Compilation ---\n",
    "print(\"\\n--- 2. JIT Compilation Phase ---\")\n",
    "jit_op = jax.jit(massive_loop_operation)\n",
    "\n",
    "# Trigger compilation. This is the \"First Run\" for the JIT function.\n",
    "# It includes tracing + compiling optimization. It is usually slow.\n",
    "start_compile = time.time()\n",
    "_ = jit_op(data_A).block_until_ready()\n",
    "print(f\"[Compilation (Data A)] Time: {time.time() - start_compile:.5f} s\")\n",
    "\n",
    "# --- Scenario 3: After JIT (The Payoff) ---\n",
    "print(\"\\n--- 3. Optimized Execution (JIT) ---\")\n",
    "\n",
    "# Re-run Data A (Should hit cache)\n",
    "# Note: measure_time func already does a 'warmup' run internally, \n",
    "# ensuring we are measuring pure execution speed.\n",
    "t_jit_A = measure_time(jit_op, data_A, \"JIT - Data A\")\n",
    "\n",
    "# Run Data B (Should ALSO hit cache if shapes match)\n",
    "t_jit_B = measure_time(jit_op, data_B, \"JIT - Data B\")\n",
    "\n",
    "# --- Final Comparison ---\n",
    "print(\"\\n==========================================\")\n",
    "print(\"             FINAL RESULTS                \")\n",
    "print(\"==========================================\")\n",
    "\n",
    "speedup_A = t_raw_A / t_jit_A\n",
    "speedup_B = t_raw_B / t_jit_B\n",
    "\n",
    "print(f\"Data A Speedup: {speedup_A:.2f}x  (Raw: {t_raw_A:.4f}s -> JIT: {t_jit_A:.4f}s)\")\n",
    "print(f\"Data B Speedup: {speedup_B:.2f}x  (Raw: {t_raw_B:.4f}s -> JIT: {t_jit_B:.4f}s)\")\n",
    "\n",
    "if speedup_B > 5.0:\n",
    "    print(\"\\nâœ… Conclusion: MASSIVE success. The loop fusion worked perfectly.\")\n",
    "elif speedup_B > 1.2:\n",
    "    print(\"\\nâš ï¸ Conclusion: Moderate success. Some overhead was removed.\")\n",
    "else:\n",
    "    print(\"\\nâŒ Conclusion: Failure. No significant gain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851d79e",
   "metadata": {},
   "source": [
    "# vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44d33c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch result: [ 5 25 61]\n"
     ]
    }
   ],
   "source": [
    "def vmap_example():\n",
    "    def vector_squared_sum(vec):\n",
    "        return jnp.sum(vec ** 2)\n",
    "    \n",
    "    batch = jnp.array([[1, 2], [3, 4], [5, 6]])\n",
    "    batch_squared_sum = vmap(vector_squared_sum)\n",
    "\n",
    "    result = batch_squared_sum(batch)\n",
    "    print(\"Batch result:\", result)\n",
    "\n",
    "vmap_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1edeb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch Size: 5000, Vector Dim: 500\n",
      "---------------------------------------------------\n",
      "Running Python Loop (Naively iterating)...\n",
      "[Python Loop] Time: 0.92591 s\n",
      "\n",
      "Running vmap (Vectorized)...\n",
      "[JAX vmap] Time: 0.00029 s\n",
      "\n",
      "---------------------------------------------------\n",
      "Speedup: 3220.19x faster\n",
      "ðŸš€ Result: Massive speedup! vmap eliminated the loop overhead.\n"
     ]
    }
   ],
   "source": [
    "## speed test\n",
    "\n",
    "# --- 1. Setup Data ---\n",
    "BATCH_SIZE = 5000  # Large enough to make the loop painful\n",
    "DIM = 500          # Dimension of each single sample\n",
    "\n",
    "# A batch of data: shape (5000, 500)\n",
    "# This represents 5000 independent samples\n",
    "data_batch = jax.random.normal(jax.random.PRNGKey(0), (BATCH_SIZE, DIM))\n",
    "# A weight matrix shared across all samples\n",
    "weights = jax.random.normal(jax.random.PRNGKey(1), (DIM, DIM))\n",
    "\n",
    "# --- 2. Define the Single-Sample Operation ---\n",
    "# This function is written to handle ONE sample (shape: DIM)\n",
    "# It simulates a dense layer + activation + normalization\n",
    "def process_single_sample(sample):\n",
    "    # Matrix multiplication: (DIM) @ (DIM, DIM) -> (DIM)\n",
    "    x = jnp.dot(sample, weights)\n",
    "    # Activation\n",
    "    x = jnp.tanh(x)\n",
    "    # Some extra math (normalization-ish)\n",
    "    return x / (jnp.linalg.norm(x) + 1e-6)\n",
    "\n",
    "# --- 3. Benchmark Helper ---\n",
    "def measure(func, *args, name=\"\"):\n",
    "    # Warmup\n",
    "    _ = func(*args).block_until_ready()\n",
    "    \n",
    "    start = time.time()\n",
    "    res = func(*args)\n",
    "    res.block_until_ready()\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"[{name}] Time: {end - start:.5f} s\")\n",
    "    return end - start\n",
    "\n",
    "print(f\"Processing Batch Size: {BATCH_SIZE}, Vector Dim: {DIM}\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "# --- Scenario 1: The Python Loop (The 'Wrong' Way) ---\n",
    "# We iterate through the batch in Python and apply the function one by one.\n",
    "# Even if we JIT the single function, the loop overhead kills performance.\n",
    "@jax.jit\n",
    "def fast_single_op(x):\n",
    "    return process_single_sample(x)\n",
    "\n",
    "def python_loop_approach(batch):\n",
    "    # Using a list comprehension to loop over the 5000 samples\n",
    "    return jnp.stack([fast_single_op(sample) for sample in batch])\n",
    "\n",
    "print(\"Running Python Loop (Naively iterating)...\")\n",
    "# Note: This might take a few seconds\n",
    "t_loop = measure(python_loop_approach, data_batch, name=\"Python Loop\")\n",
    "\n",
    "\n",
    "# --- Scenario 2: JAX vmap (The 'Right' Way) ---\n",
    "# vmap takes the single-sample function and transforms it \n",
    "# into a function that accepts a batch.\n",
    "# We also JIT the vmapped function to fuse everything into one kernel.\n",
    "vmapped_op = jax.jit(jax.vmap(process_single_sample))\n",
    "\n",
    "print(\"\\nRunning vmap (Vectorized)...\")\n",
    "t_vmap = measure(vmapped_op, data_batch, name=\"JAX vmap\")\n",
    "\n",
    "\n",
    "# --- Result ---\n",
    "print(\"\\n---------------------------------------------------\")\n",
    "print(f\"Speedup: {t_loop / t_vmap:.2f}x faster\")\n",
    "\n",
    "if t_loop / t_vmap > 10:\n",
    "    print(\"ðŸš€ Result: Massive speedup! vmap eliminated the loop overhead.\")\n",
    "else:\n",
    "    print(\"ðŸ¤” Result: Not fast enough? Try increasing BATCH_SIZE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc421d",
   "metadata": {},
   "source": [
    "## MNIST Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f776f36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:01<00:00, 6.62MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 262kB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 2.61MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 20.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training...\n",
      "Notice the time difference between the VERY first step and the rest.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36391/3567236049.py:106: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  lambda x: np.array(x).transpose(1, 2, 0) # Format: (H, W, C) -> (28, 28, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 1 Time: 3.5787s (COMPILING...) | Loss: 2.3111\n",
      "Epoch 1, Step 2 Time: 0.0027s (Running Cached) | Loss: 2.1591\n",
      "Epoch 1, Step 3 Time: 0.0022s (Running Cached) | Loss: 2.0437\n",
      "Epoch 1, Step 101 Time: 0.0023s | Acc: 93.75%\n",
      "Epoch 1, Step 201 Time: 0.0022s | Acc: 92.97%\n",
      "Epoch 1, Step 301 Time: 0.0023s | Acc: 96.88%\n",
      "Epoch 1, Step 401 Time: 0.0022s | Acc: 98.44%\n",
      "Epoch 1 finished in 6.56s\n",
      "------------------------------------------------------------\n",
      "Epoch 2, Step 1 Time: 0.0023s | Acc: 99.22%\n",
      "Epoch 2, Step 101 Time: 0.0022s | Acc: 98.44%\n",
      "Epoch 2, Step 201 Time: 0.0022s | Acc: 98.44%\n",
      "Epoch 2, Step 301 Time: 0.0023s | Acc: 97.66%\n",
      "Epoch 2, Step 401 Time: 0.0022s | Acc: 98.44%\n",
      "Epoch 2 finished in 2.97s\n",
      "------------------------------------------------------------\n",
      "Epoch 3, Step 1 Time: 0.0023s | Acc: 98.44%\n",
      "Epoch 3, Step 101 Time: 0.0023s | Acc: 99.22%\n",
      "Epoch 3, Step 201 Time: 0.0023s | Acc: 99.22%\n",
      "Epoch 3, Step 301 Time: 0.0022s | Acc: 100.00%\n",
      "Epoch 3, Step 401 Time: 0.0023s | Acc: 100.00%\n",
      "Epoch 3 finished in 2.97s\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# We use PyTorch/Torchvision JUST for easy data downloading/loading.\n",
    "# In a pure JAX workflow, you might use TensorFlow Datasets, but this is familiar.\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- 1. Define the CNN Architecture (Using Flax) ---\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Convolutional Neural Network for MNIST.\n",
    "    \"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x shape: [batch, 28, 28, 1]\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        # Flatten the implementation by reshaping\n",
    "        x = x.reshape((x.shape[0], -1)) \n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x) # Output: 10 digits\n",
    "        return x\n",
    "\n",
    "# --- 2. Helper Functions ---\n",
    "def numpy_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to convert PyTorch tensors to Numpy arrays\n",
    "    compatible with JAX.\n",
    "    \"\"\"\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    \"\"\"\n",
    "    A simple container to hold the network state (parameters + optimizer).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, learning_rate):\n",
    "    \"\"\"Initialize the model and optimizer.\"\"\"\n",
    "    cnn = CNN()\n",
    "    # Initialize parameters with a dummy input (shape: [1, 28, 28, 1])\n",
    "    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)\n",
    "\n",
    "# --- 3. The Core JAX Training Step (The \"Magic\" Part) ---\n",
    "# @jax.jit tells JAX to compile this ENTIRE function into XLA code.\n",
    "@jax.jit\n",
    "def train_step(state, batch_images, batch_labels):\n",
    "    \"\"\"\n",
    "    Runs a single training step: Forward -> Loss -> Gradient -> Update.\n",
    "    \"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch_images)\n",
    "        # Calculate Cross Entropy Loss\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits=logits, labels=batch_labels\n",
    "        ).mean()\n",
    "        return loss, logits\n",
    "\n",
    "    # Get gradients using JAX's auto-differentiation\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    \n",
    "    # Update the state (parameters) using the optimizer\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    \n",
    "    # Calculate simple accuracy\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch_labels)\n",
    "    }\n",
    "    return state, metrics\n",
    "\n",
    "# --- 4. Main Execution Loop ---\n",
    "def run_training():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 3\n",
    "    \n",
    "    # Load Data (Using Torchvision, converted to Numpy)\n",
    "    # Transforming to tensor then numpy array, adding channel dimension\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: np.array(x).transpose(1, 2, 0) # Format: (H, W, C) -> (28, 28, 1)\n",
    "    ])\n",
    "    \n",
    "    train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                              shuffle=True, collate_fn=numpy_collate, drop_last=True)\n",
    "\n",
    "    # Initialize State\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    state = create_train_state(rng, LEARNING_RATE)\n",
    "    \n",
    "    print(\"\\nStarting Training...\")\n",
    "    print(\"Notice the time difference between the VERY first step and the rest.\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for i, (batch_images, batch_labels) in enumerate(train_loader):\n",
    "            step_start = time.time()\n",
    "            \n",
    "            # --- THE MOMENT OF TRUTH ---\n",
    "            # The first time this is called, JAX compiles the graph (slow).\n",
    "            # Subsequent calls reuse the compiled kernel (fast).\n",
    "            state, metrics = train_step(state, batch_images, batch_labels)\n",
    "            \n",
    "            # Use block_until_ready() to get accurate timing for benchmarks\n",
    "            metrics['loss'].block_until_ready() \n",
    "            step_end = time.time()\n",
    "            \n",
    "            if epoch == 0 and i < 3:\n",
    "                # Log the first few steps explicitly to show compilation hit\n",
    "                print(f\"Epoch {epoch+1}, Step {i+1} Time: {step_end - step_start:.4f}s \"\n",
    "                      f\"{'(COMPILING...)' if i==0 else '(Running Cached)'} | \"\n",
    "                      f\"Loss: {metrics['loss']:.4f}\")\n",
    "            elif i % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {i+1} Time: {step_end - step_start:.4f}s | \"\n",
    "                      f\"Acc: {metrics['accuracy']:.2%}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} finished in {time.time() - epoch_start:.2f}s\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_rocm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
