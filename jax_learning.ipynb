{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437e04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import jit, grad, vmap\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322949cc",
   "metadata": {},
   "source": [
    "# jax/numpy difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24800cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Array: [0 1 2 3 4]\n",
      "Original JAX Array: [0 1 2 3 4]\n",
      "Updated JAX Array: [10  1  2  3  4]\n"
     ]
    }
   ],
   "source": [
    "def jax_basics():\n",
    "    x_np = np.arange(5)\n",
    "    x_jax = jnp.arange(5)\n",
    "\n",
    "    print(\"JAX Array:\", x_jax)\n",
    "    #x_jax[0] = 10 # illegal operation\n",
    "    y_jax = x_jax.at[0].set(10)\n",
    "\n",
    "    print(\"Original JAX Array:\", x_jax)\n",
    "    print(\"Updated JAX Array:\", y_jax)\n",
    "\n",
    "jax_basics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e7b98",
   "metadata": {},
   "source": [
    "# grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d44d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = 1/(x^2), x = 2.0\n",
      "Gradient (-2/(x^3)) result: -0.25\n"
     ]
    }
   ],
   "source": [
    "def derivative_example(input_val):\n",
    "    def f(x):\n",
    "        return 1/(x**2)\n",
    "    \n",
    "    df_dx = grad(f)\n",
    "\n",
    "    result = df_dx(input_val)\n",
    "\n",
    "    print(f\"f(x) = 1/(x^2), x = {input_val}\")\n",
    "    print(f\"Gradient (-2/(x^3)) result: {result}\")\n",
    "\n",
    "derivative_example(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13269753",
   "metadata": {},
   "source": [
    "# jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7df43c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without JIT: 0.0144 seconds\n",
      "With JIT:    0.0115 seconds\n"
     ]
    }
   ],
   "source": [
    "def heavy_computation(x):\n",
    "    return jnp.dot(x, x.T)\n",
    "\n",
    "def jit_example():\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    x = jax.random.normal(key, (5000,5000))\n",
    "\n",
    "    start = time.time()\n",
    "    heavy_computation(x).block_until_ready() ## make sure \n",
    "    print(f\"Without JIT: {time.time() - start:.4f} seconds\")\n",
    "\n",
    "    fast_computation = jit(heavy_computation)\n",
    "    fast_computation(x).block_until_ready()\n",
    "\n",
    "    start = time.time()\n",
    "    fast_computation(x).block_until_ready()\n",
    "    print(f\"With JIT:    {time.time() - start:.4f} seconds\")\n",
    "\n",
    "jit_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b74b3bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "   JAX BENCHMARK: No-JIT vs. JIT\n",
      "==========================================\n",
      "\n",
      "--- 1. Baseline (No JIT / Eager) ---\n",
      "[No JIT - Data A] Time: 0.10474 s\n",
      "[No JIT - Data B] Time: 0.10433 s\n",
      "\n",
      "--- 2. JIT Compilation Phase ---\n",
      "[Compilation (Data A)] Time: 0.25615 s\n",
      "\n",
      "--- 3. Optimized Execution (JIT) ---\n",
      "[JIT - Data A] Time: 0.00111 s\n",
      "[JIT - Data B] Time: 0.00114 s\n",
      "\n",
      "==========================================\n",
      "             FINAL RESULTS                \n",
      "==========================================\n",
      "Data A Speedup: 94.53x  (Raw: 0.1047s -> JIT: 0.0011s)\n",
      "Data B Speedup: 91.89x  (Raw: 0.1043s -> JIT: 0.0011s)\n",
      "\n",
      "âœ… Conclusion: MASSIVE success. The loop fusion worked perfectly.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# --- 1. Setup Data ---\n",
    "# We use a reasonably sized array\n",
    "N = 2000\n",
    "data_A = jax.random.normal(jax.random.PRNGKey(0), (N, N))\n",
    "data_B = jax.random.normal(jax.random.PRNGKey(1), (N, N))\n",
    "\n",
    "# --- 2. The Logic: \"Loop Hell\" ---\n",
    "# Why this?\n",
    "# Without JIT: Python dispatches op-by-op. 100 loops * 3 ops = 300 overheads.\n",
    "# With JIT: XLA fuses all 300 ops into ONE single GPU/CPU kernel.\n",
    "def massive_loop_operation(x):\n",
    "    res = x\n",
    "    # A loop that is painful for Python interpreter but easy for XLA\n",
    "    for _ in range(100): \n",
    "        res = res - 0.001 * jnp.sin(res) # Element-wise math\n",
    "        res = jnp.where(res > 1.0, 1.0, res) # Conditional logic\n",
    "    return jnp.sum(res)\n",
    "\n",
    "# --- 3. Benchmark Helper (With Explicit Warmup) ---\n",
    "def measure_time(func, data, name):\n",
    "    # 1. Warmup / Dry Run (User's logic: \"Run once, disregard result\")\n",
    "    # This initializes any lazy allocations or caches\n",
    "    _ = func(data).block_until_ready()\n",
    "    \n",
    "    # 2. Actual Measurement\n",
    "    # Now that the system is \"warm\", we measure the second run\n",
    "    start = time.time()\n",
    "    res = func(data)\n",
    "    res.block_until_ready() # CRITICAL: Wait for async computation\n",
    "    end = time.time()\n",
    "    \n",
    "    duration = end - start\n",
    "    print(f\"[{name}] Time: {duration:.5f} s\")\n",
    "    return duration\n",
    "\n",
    "print(\"==========================================\")\n",
    "print(\"   JAX BENCHMARK: No-JIT vs. JIT\")\n",
    "print(\"==========================================\\n\")\n",
    "\n",
    "# --- Scenario 1: No JIT (Baseline) ---\n",
    "# We explicitly check BOTH Data A and Data B without JIT\n",
    "with jax.disable_jit():\n",
    "    print(\"--- 1. Baseline (No JIT / Eager) ---\")\n",
    "    t_raw_A = measure_time(massive_loop_operation, data_A, \"No JIT - Data A\")\n",
    "    t_raw_B = measure_time(massive_loop_operation, data_B, \"No JIT - Data B\")\n",
    "\n",
    "# --- Scenario 2: JIT Compilation ---\n",
    "print(\"\\n--- 2. JIT Compilation Phase ---\")\n",
    "jit_op = jax.jit(massive_loop_operation)\n",
    "\n",
    "# Trigger compilation. This is the \"First Run\" for the JIT function.\n",
    "# It includes tracing + compiling optimization. It is usually slow.\n",
    "start_compile = time.time()\n",
    "_ = jit_op(data_A).block_until_ready()\n",
    "print(f\"[Compilation (Data A)] Time: {time.time() - start_compile:.5f} s\")\n",
    "\n",
    "# --- Scenario 3: After JIT (The Payoff) ---\n",
    "print(\"\\n--- 3. Optimized Execution (JIT) ---\")\n",
    "\n",
    "# Re-run Data A (Should hit cache)\n",
    "# Note: measure_time func already does a 'warmup' run internally, \n",
    "# ensuring we are measuring pure execution speed.\n",
    "t_jit_A = measure_time(jit_op, data_A, \"JIT - Data A\")\n",
    "\n",
    "# Run Data B (Should ALSO hit cache if shapes match)\n",
    "t_jit_B = measure_time(jit_op, data_B, \"JIT - Data B\")\n",
    "\n",
    "# --- Final Comparison ---\n",
    "print(\"\\n==========================================\")\n",
    "print(\"             FINAL RESULTS                \")\n",
    "print(\"==========================================\")\n",
    "\n",
    "speedup_A = t_raw_A / t_jit_A\n",
    "speedup_B = t_raw_B / t_jit_B\n",
    "\n",
    "print(f\"Data A Speedup: {speedup_A:.2f}x  (Raw: {t_raw_A:.4f}s -> JIT: {t_jit_A:.4f}s)\")\n",
    "print(f\"Data B Speedup: {speedup_B:.2f}x  (Raw: {t_raw_B:.4f}s -> JIT: {t_jit_B:.4f}s)\")\n",
    "\n",
    "if speedup_B > 5.0:\n",
    "    print(\"\\nâœ… Conclusion: MASSIVE success. The loop fusion worked perfectly.\")\n",
    "elif speedup_B > 1.2:\n",
    "    print(\"\\nâš ï¸ Conclusion: Moderate success. Some overhead was removed.\")\n",
    "else:\n",
    "    print(\"\\nâŒ Conclusion: Failure. No significant gain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851d79e",
   "metadata": {},
   "source": [
    "# vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d33c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch result: [ 5 25 61]\n"
     ]
    }
   ],
   "source": [
    "def vmap_example():\n",
    "    def vector_squared_sum(vec):\n",
    "        return jnp.sum(vec ** 2)\n",
    "    \n",
    "    batch = jnp.array([[1, 2], [3, 4], [5, 6]])\n",
    "    batch_squared_sum = vmap(vector_squared_sum)\n",
    "\n",
    "    result = batch_squared_sum(batch)\n",
    "    print(\"Batch result:\", result)\n",
    "\n",
    "vmap_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1edeb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch Size: 5000, Vector Dim: 500\n",
      "---------------------------------------------------\n",
      "Running Python Loop (Naively iterating)...\n",
      "[Python Loop] Time: 0.92129 s\n",
      "\n",
      "Running vmap (Vectorized)...\n",
      "[JAX vmap] Time: 0.00030 s\n",
      "\n",
      "---------------------------------------------------\n",
      "Speedup: 3081.47x faster\n",
      "ðŸš€ Result: Massive speedup! vmap eliminated the loop overhead.\n"
     ]
    }
   ],
   "source": [
    "## speed test\n",
    "\n",
    "# --- 1. Setup Data ---\n",
    "BATCH_SIZE = 5000  # Large enough to make the loop painful\n",
    "DIM = 500          # Dimension of each single sample\n",
    "\n",
    "# A batch of data: shape (5000, 500)\n",
    "# This represents 5000 independent samples\n",
    "data_batch = jax.random.normal(jax.random.PRNGKey(0), (BATCH_SIZE, DIM))\n",
    "# A weight matrix shared across all samples\n",
    "weights = jax.random.normal(jax.random.PRNGKey(1), (DIM, DIM))\n",
    "\n",
    "# --- 2. Define the Single-Sample Operation ---\n",
    "# This function is written to handle ONE sample (shape: DIM)\n",
    "# It simulates a dense layer + activation + normalization\n",
    "def process_single_sample(sample):\n",
    "    # Matrix multiplication: (DIM) @ (DIM, DIM) -> (DIM)\n",
    "    x = jnp.dot(sample, weights)\n",
    "    # Activation\n",
    "    x = jnp.tanh(x)\n",
    "    # Some extra math (normalization-ish)\n",
    "    return x / (jnp.linalg.norm(x) + 1e-6)\n",
    "\n",
    "# --- 3. Benchmark Helper ---\n",
    "def measure(func, *args, name=\"\"):\n",
    "    # Warmup\n",
    "    _ = func(*args).block_until_ready()\n",
    "    \n",
    "    start = time.time()\n",
    "    res = func(*args)\n",
    "    res.block_until_ready()\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"[{name}] Time: {end - start:.5f} s\")\n",
    "    return end - start\n",
    "\n",
    "print(f\"Processing Batch Size: {BATCH_SIZE}, Vector Dim: {DIM}\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "# --- Scenario 1: The Python Loop (The 'Wrong' Way) ---\n",
    "# We iterate through the batch in Python and apply the function one by one.\n",
    "# Even if we JIT the single function, the loop overhead kills performance.\n",
    "@jax.jit\n",
    "def fast_single_op(x):\n",
    "    return process_single_sample(x)\n",
    "\n",
    "def python_loop_approach(batch):\n",
    "    # Using a list comprehension to loop over the 5000 samples\n",
    "    return jnp.stack([fast_single_op(sample) for sample in batch])\n",
    "\n",
    "print(\"Running Python Loop (Naively iterating)...\")\n",
    "# Note: This might take a few seconds\n",
    "t_loop = measure(python_loop_approach, data_batch, name=\"Python Loop\")\n",
    "\n",
    "\n",
    "# --- Scenario 2: JAX vmap (The 'Right' Way) ---\n",
    "# vmap takes the single-sample function and transforms it \n",
    "# into a function that accepts a batch.\n",
    "# We also JIT the vmapped function to fuse everything into one kernel.\n",
    "vmapped_op = jax.jit(jax.vmap(process_single_sample))\n",
    "\n",
    "print(\"\\nRunning vmap (Vectorized)...\")\n",
    "t_vmap = measure(vmapped_op, data_batch, name=\"JAX vmap\")\n",
    "\n",
    "\n",
    "# --- Result ---\n",
    "print(\"\\n---------------------------------------------------\")\n",
    "print(f\"Speedup: {t_loop / t_vmap:.2f}x faster\")\n",
    "\n",
    "if t_loop / t_vmap > 10:\n",
    "    print(\"ðŸš€ Result: Massive speedup! vmap eliminated the loop overhead.\")\n",
    "else:\n",
    "    print(\"ðŸ¤” Result: Not fast enough? Try increasing BATCH_SIZE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc421d",
   "metadata": {},
   "source": [
    "# MNIST Training Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c0eff",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4b9cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42911/2112103967.py:102: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  lambda x: np.array(x).transpose(1, 2, 0) # Convert (C, H, W) -> (H, W, C)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model state is ready for inference.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- 1. Network Architecture ---\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network architecture for MNIST digit classification.\n",
    "    Input shape: (Batch, 28, 28, 1)\n",
    "    Output shape: (Batch, 10)\n",
    "    \"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # First convolutional block: Conv -> ReLU -> AvgPool\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        \n",
    "        # Flatten and Dense layers for classification\n",
    "        x = x.reshape((x.shape[0], -1)) \n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        return x\n",
    "\n",
    "# --- 2. State Management ---\n",
    "class TrainState(train_state.TrainState):\n",
    "    \"\"\"\n",
    "    Custom TrainState to hold model parameters and optimizer state.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, learning_rate):\n",
    "    \"\"\"\n",
    "    Initializes the model parameters and the optimizer.\n",
    "    \"\"\"\n",
    "    cnn = CNN()\n",
    "    # Initialize parameters with a dummy input of the correct shape\n",
    "    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)\n",
    "\n",
    "# --- 3. JIT-Compiled Training Step ---\n",
    "@jax.jit\n",
    "def train_step(state, batch_images, batch_labels):\n",
    "    \"\"\"\n",
    "    Performs a single training step: forward pass, loss calculation, and parameter update.\n",
    "    This function is JIT-compiled by XLA for high performance.\n",
    "    \"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch_images)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits=logits, labels=batch_labels\n",
    "        ).mean()\n",
    "        return loss, logits\n",
    "\n",
    "    # Compute gradients via automatic differentiation\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "    \n",
    "    # Update model parameters using the calculated gradients\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# --- 4. Data Helper ---\n",
    "def numpy_collate(batch):\n",
    "    \"\"\"\n",
    "    Converts PyTorch data loader batches into Numpy arrays for JAX compatibility.\n",
    "    \"\"\"\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "# --- 5. Main Training Execution ---\n",
    "def train_and_return_state():\n",
    "    \"\"\"\n",
    "    Loads data, initializes the model, runs the training loop, and returns the final state.\n",
    "    \"\"\"\n",
    "    # Hyperparameters definition\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 3\n",
    "    \n",
    "    # Prepare data pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: np.array(x).transpose(1, 2, 0) # Convert (C, H, W) -> (H, W, C)\n",
    "    ])\n",
    "    \n",
    "    dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                            collate_fn=numpy_collate, drop_last=True)\n",
    "    \n",
    "    # Initialize model state\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    state = create_train_state(rng, LEARNING_RATE)\n",
    "    \n",
    "    print(\"Initializing training loop...\")\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            state, _ = train_step(state, batch_images, batch_labels)\n",
    "            \n",
    "    print(\"Training complete. Model state is ready for inference.\")\n",
    "    return state\n",
    "\n",
    "# Execute training and store the result in a global variable\n",
    "trained_state = train_and_return_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b37d1",
   "metadata": {},
   "source": [
    "## Inference Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d440c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict_digit(image):\n",
    "    \"\"\"\n",
    "    Preprocesses the input image and runs inference using the trained JAX model.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(image, dict):\n",
    "        image = image[\"composite\"]\n",
    "        \n",
    "    # Return None if the input canvas is empty\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # 1. Image Preprocessing\n",
    "    # Convert the input array (from Gradio) to a PIL Image for easier manipulation.\n",
    "    # 'L' mode converts it to grayscale.\n",
    "    img_pil = Image.fromarray(image).convert('L')\n",
    "    \n",
    "    # Resize the image to match the model's expected input size (MNIST is 28x28).\n",
    "    img_pil = img_pil.resize((28, 28))\n",
    "    \n",
    "    # Convert back to a numpy array for mathematical operations.\n",
    "    img_arr = np.array(img_pil)\n",
    "    \n",
    "    # Invert colors: \n",
    "    # The drawing canvas is white background (255) with black ink (0).\n",
    "    # The MNIST dataset is black background (0) with white ink (255).\n",
    "    # We perform '255 - pixel_value' to match the training data distribution.\n",
    "    img_arr = 255.0 - img_arr\n",
    "    \n",
    "    # Normalize pixel values to the [0, 1] range.\n",
    "    img_arr = img_arr / 255.0\n",
    "    \n",
    "    # Reshape to add the batch dimension: (H, W, C) -> (1, H, W, C).\n",
    "    img_arr = img_arr.reshape(1, 28, 28, 1)\n",
    "    \n",
    "    # 2. Model Inference\n",
    "    # Use the global 'trained_state' to compute logits.\n",
    "    logits = trained_state.apply_fn({'params': trained_state.params}, img_arr)\n",
    "    \n",
    "    # Apply softmax to convert raw logits into probabilities.\n",
    "    probs = jax.nn.softmax(logits).squeeze()\n",
    "    \n",
    "    # Return a dictionary mapping each digit (0-9) to its probability.\n",
    "    return {str(i): float(probs[i]) for i in range(10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdb93b",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "400cac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "# Assuming predict_digit is defined above\n",
    "\n",
    "# Define the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=predict_digit, \n",
    "    # Note: In Gradio 4.x, Sketchpad might return a dictionary. \n",
    "    # If 'predict_digit' fails later, check if 'inputs' needs to be gr.Image(source=\"canvas\")\n",
    "    inputs=gr.Sketchpad(label=\"Draw a Digit\", type=\"numpy\"),\n",
    "    outputs=gr.Label(num_top_classes=3, label=\"Prediction\"),\n",
    "    title=\"JAX Hand-written Digit Recognizer\",\n",
    "    description=\"Draw a number on the pad. The JAX model will predict the digit.\",\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # FIX: Gradio 4.0+ uses 'flagging_mode' instead of 'allow_flagging'\n",
    "    # Options are: \"manual\", \"auto\", or \"never\"\n",
    "    # ---------------------------------------------------------\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the web server\n",
    "demo.launch(share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_rocm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
